 

* cost to complexity: "to avoid overfitting"


* `Conventional statistical and econometric techniques`: regression often work well, but there are issues unique to big datasets that may require different tools. We need tools for:
Problems:
1) powerful data manipulation tools
2) Large data mena more features. We need to see which ones are "relevant" else noise causes "overfitting" and poor "generalisation"
3) Third,large datasets may allow for more for more flexible relationships than simple linear models. 

### Data manipulation tools
- MySQL. Relational databases offer a flexible way to store, manipulate, and retrieve
data using a Structured Query Language (SQL): medium-sized datasets.
- several gigabytes: more primitive than SQL: can handle larger amounts of data
- A number of these tools are proprietary to Google, but have been described in
academic publications in sufficient detail that open-source implementations have been developed: need Amazon, Google, pplications use large clusters of computerssuch asthose provided byAmazon, Google,
Microsoft, and other cloud-computing providers.
- The outcome of the big-data processing described above is often a “small” table of data that may be directly human readable or can be loaded into an SQL database, a statistics package, or a spreadsheet
- For even larger data: At Google, for example, I have found that random samples on the order of 0.1 percent work fine for analysis of business data. Next: EDA, consistency and data-cleaning tasks.
- Exploratory Data Analysis (EDA) is an approach to analyzing and visualizing data sets to summarize their main characteristics, often with the help of statistical graphics and other data visualization methods.

- OpenRefine and DataWrangler can be used to assist in data cleansing.
- Data analysis in statistics and econometrics can be broken down into four categories: 1) prediction, 2) summarization, 3) estimation, and 4) hypothesis testing.
- ML: prediction , Data Mining:  summarization finding interesting patterns
- Econometricians, statisticians, and data mining specialists are generally looking for insights. Econometricians need to think about how will this policy affect the society

- Words: knowledge extraction, information discovery, information harvesting, 
data archaeology, data pattern processing, and exploratory data analysis

- econometrics is concerned with detecting and summarizing relationships in the data

the conditional distribution of some variable nterested in understanding the conditional distribution of some variable y given
some other variables ome other variables x = (x 1 , … ,x P ). If we want a point prediction, we can use the . If we want a point prediction, we can use the
mean or median of the conditional distribution. That actually the the thing behind Linear Regression. What we predict is the mean of a Normally Distributed RV


- fat: lots of predictors relative to the number of observatiuons
- “tall”:  lots of observations relative to the number of predictors.

- “good” means it minimizes Usually “good” means it minimizes some loss function such as the sum of squared residuals,
- When confronted with a prediction problem of this sort an economist would think immediately of a linear or logistic regression
- other methids: 
 1) classification and regression trees (CART);
 2) random forests; 
 3) penalized regression such as LASSO, LARS, and elastic nets.







# How to grow the tree?

* Continuous are split based on median, Catrgorial :k way split
* How you build the tree in this image is not explined but I'll  briefly explain how it's done: You try splitting over every paramater and then choose the one that's associated with tthe maximum information exchange. I'll take these 2 simple diagrams to explain this, w/o needing to use the math. Just to give the idea.

* As an example, let us continue with  the Titanic data and create a data and create a
tree that relates survival to age. In this case, the rule generated by the tree is \
simple: predict “survive” if age < 8.5 years. We can examine the same data with a 8.5 years. We can examine the same data with a
logistic regression to estimate the probability of survival as a function of age, (Not sure how this rule came. The median age is 8.5)
results reported in Table 3. 

* t values and p values?
* The tree model suggests that age is an important predictor of survival, while the logistic model says it is barely important

[5 rows x 14 columns]
Index(['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket',
       'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],
      dtype='object')
Median:  28.0
Mean:  29.881137667304014
min age: 0.17 val: 0.6141190737792076
max age: 80.0 val: 0.5756643510425494


* Not sure how usage of the age was rendered useless

the age example shows that they may reveal aspects of the data that are not apparent from a traditional linear modeling approach.
* Trees also handle missing data well

* Perlich, Provost, and Simonoff (2003) rees also handle missing data well. 
examined several standard datasets and found that  “logistic regression is better for smaller data sets and tree induction for larger data sets.”
* r the tree formulation made this nonlinearity immediately apparent.


## Growing the tree:
* pure leaf idea 
* splitting then the lower level boxes
* now let's not discuss the splitting criuteria is chosen


* 
* Not clearly mentioned what p < 0.001 and p  = 0.01 means 






 
* see image while dmo is strong 


le summaries of relationships in the data


I ran the random forest method on the HMDA data and found that it misclassi- ran the random forest method on the HMDA data and found that it misclassifi ed 223 of the 2,380 cases, a small improvement over the logit and the ctree. I also ed 223 of the 2,380 cases, a small improvement over the logit and the ctree. I also
used the importance option in random forests to see how the predictors compared. sed the importance option in random forests to see how the predictors compared.
It turned out that “dmi t turned out that “dmi” was the most important predictor and race was second was the most important predictor and race was second
from the bottom, which is consistent with the ctree analysis


